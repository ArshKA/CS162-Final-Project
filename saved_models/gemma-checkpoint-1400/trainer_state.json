{
  "best_global_step": 1400,
  "best_metric": 0.9960774400000001,
  "best_model_checkpoint": "./saved_models/mistral_raid_detector_adapter_gemma7b/checkpoint-1400",
  "epoch": 0.9955555555555555,
  "eval_steps": 140,
  "global_step": 1400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035555555555555557,
      "grad_norm": 70.26992797851562,
      "learning_rate": 1.1363636363636364e-07,
      "loss": 3.4878,
      "step": 5
    },
    {
      "epoch": 0.0071111111111111115,
      "grad_norm": 95.11664581298828,
      "learning_rate": 2.556818181818182e-07,
      "loss": 4.2036,
      "step": 10
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 154.87937927246094,
      "learning_rate": 3.9772727272727276e-07,
      "loss": 3.8348,
      "step": 15
    },
    {
      "epoch": 0.014222222222222223,
      "grad_norm": 82.56410217285156,
      "learning_rate": 5.397727272727273e-07,
      "loss": 2.899,
      "step": 20
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 117.05858612060547,
      "learning_rate": 6.818181818181818e-07,
      "loss": 3.5184,
      "step": 25
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 203.56687927246094,
      "learning_rate": 8.238636363636364e-07,
      "loss": 3.4394,
      "step": 30
    },
    {
      "epoch": 0.024888888888888887,
      "grad_norm": 109.56822967529297,
      "learning_rate": 9.65909090909091e-07,
      "loss": 3.6222,
      "step": 35
    },
    {
      "epoch": 0.028444444444444446,
      "grad_norm": 76.61902618408203,
      "learning_rate": 1.1079545454545456e-06,
      "loss": 3.1086,
      "step": 40
    },
    {
      "epoch": 0.032,
      "grad_norm": 145.2755126953125,
      "learning_rate": 1.25e-06,
      "loss": 3.905,
      "step": 45
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 96.07048797607422,
      "learning_rate": 1.3920454545454546e-06,
      "loss": 3.1775,
      "step": 50
    },
    {
      "epoch": 0.03911111111111111,
      "grad_norm": 63.15557098388672,
      "learning_rate": 1.5340909090909093e-06,
      "loss": 3.4269,
      "step": 55
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 153.05435180664062,
      "learning_rate": 1.6761363636363636e-06,
      "loss": 3.445,
      "step": 60
    },
    {
      "epoch": 0.04622222222222222,
      "grad_norm": 204.58355712890625,
      "learning_rate": 1.8181818181818183e-06,
      "loss": 2.9976,
      "step": 65
    },
    {
      "epoch": 0.049777777777777775,
      "grad_norm": 139.3385467529297,
      "learning_rate": 1.9602272727272728e-06,
      "loss": 3.3444,
      "step": 70
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 112.7271499633789,
      "learning_rate": 2.1022727272727277e-06,
      "loss": 2.7403,
      "step": 75
    },
    {
      "epoch": 0.05688888888888889,
      "grad_norm": 126.68168640136719,
      "learning_rate": 2.2443181818181818e-06,
      "loss": 3.4501,
      "step": 80
    },
    {
      "epoch": 0.060444444444444446,
      "grad_norm": 215.10496520996094,
      "learning_rate": 2.3863636363636367e-06,
      "loss": 3.5892,
      "step": 85
    },
    {
      "epoch": 0.064,
      "grad_norm": 138.29664611816406,
      "learning_rate": 2.528409090909091e-06,
      "loss": 3.4461,
      "step": 90
    },
    {
      "epoch": 0.06755555555555555,
      "grad_norm": 290.9582214355469,
      "learning_rate": 2.6704545454545457e-06,
      "loss": 3.5543,
      "step": 95
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 273.9646301269531,
      "learning_rate": 2.8125e-06,
      "loss": 3.0624,
      "step": 100
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 209.4254913330078,
      "learning_rate": 2.954545454545455e-06,
      "loss": 2.7831,
      "step": 105
    },
    {
      "epoch": 0.07822222222222222,
      "grad_norm": 170.24923706054688,
      "learning_rate": 3.096590909090909e-06,
      "loss": 2.7144,
      "step": 110
    },
    {
      "epoch": 0.08177777777777778,
      "grad_norm": 98.15443420410156,
      "learning_rate": 3.2386363636363637e-06,
      "loss": 2.2726,
      "step": 115
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 70.88780975341797,
      "learning_rate": 3.3806818181818186e-06,
      "loss": 2.5375,
      "step": 120
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 68.78044128417969,
      "learning_rate": 3.522727272727273e-06,
      "loss": 2.514,
      "step": 125
    },
    {
      "epoch": 0.09244444444444444,
      "grad_norm": 156.1943817138672,
      "learning_rate": 3.6647727272727276e-06,
      "loss": 2.7728,
      "step": 130
    },
    {
      "epoch": 0.096,
      "grad_norm": 169.60476684570312,
      "learning_rate": 3.806818181818182e-06,
      "loss": 2.3601,
      "step": 135
    },
    {
      "epoch": 0.09955555555555555,
      "grad_norm": 80.74120330810547,
      "learning_rate": 3.9488636363636366e-06,
      "loss": 2.1241,
      "step": 140
    },
    {
      "epoch": 0.09955555555555555,
      "eval_accuracy": 0.5506,
      "eval_f1": 0.5409601634320736,
      "eval_loss": 2.536623001098633,
      "eval_precision": 0.5528183716075157,
      "eval_recall": 0.5296,
      "eval_roc_auc": 0.56521376,
      "eval_runtime": 699.2801,
      "eval_samples_per_second": 7.15,
      "eval_steps_per_second": 0.448,
      "step": 140
    },
    {
      "epoch": 0.10311111111111111,
      "grad_norm": 141.64251708984375,
      "learning_rate": 4.0909090909090915e-06,
      "loss": 2.6065,
      "step": 145
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 148.97865295410156,
      "learning_rate": 4.2329545454545455e-06,
      "loss": 2.0534,
      "step": 150
    },
    {
      "epoch": 0.11022222222222222,
      "grad_norm": 63.54043960571289,
      "learning_rate": 4.3750000000000005e-06,
      "loss": 1.8996,
      "step": 155
    },
    {
      "epoch": 0.11377777777777778,
      "grad_norm": 181.9796142578125,
      "learning_rate": 4.517045454545455e-06,
      "loss": 2.4946,
      "step": 160
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 104.0299301147461,
      "learning_rate": 4.6590909090909095e-06,
      "loss": 2.2701,
      "step": 165
    },
    {
      "epoch": 0.12088888888888889,
      "grad_norm": 203.58401489257812,
      "learning_rate": 4.8011363636363635e-06,
      "loss": 2.1098,
      "step": 170
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 211.44970703125,
      "learning_rate": 4.9431818181818184e-06,
      "loss": 1.8899,
      "step": 175
    },
    {
      "epoch": 0.128,
      "grad_norm": 62.42454147338867,
      "learning_rate": 5.085227272727273e-06,
      "loss": 1.975,
      "step": 180
    },
    {
      "epoch": 0.13155555555555556,
      "grad_norm": 92.03547668457031,
      "learning_rate": 5.2272727272727274e-06,
      "loss": 2.23,
      "step": 185
    },
    {
      "epoch": 0.1351111111111111,
      "grad_norm": 61.393882751464844,
      "learning_rate": 5.369318181818182e-06,
      "loss": 1.911,
      "step": 190
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 100.97530364990234,
      "learning_rate": 5.511363636363637e-06,
      "loss": 1.6894,
      "step": 195
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 54.43630599975586,
      "learning_rate": 5.653409090909091e-06,
      "loss": 1.6597,
      "step": 200
    },
    {
      "epoch": 0.14577777777777778,
      "grad_norm": 88.15213775634766,
      "learning_rate": 5.795454545454546e-06,
      "loss": 1.6161,
      "step": 205
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 137.56919860839844,
      "learning_rate": 5.9375e-06,
      "loss": 1.5448,
      "step": 210
    },
    {
      "epoch": 0.15288888888888888,
      "grad_norm": 70.87188720703125,
      "learning_rate": 6.079545454545454e-06,
      "loss": 1.5502,
      "step": 215
    },
    {
      "epoch": 0.15644444444444444,
      "grad_norm": 55.27814865112305,
      "learning_rate": 6.22159090909091e-06,
      "loss": 1.2987,
      "step": 220
    },
    {
      "epoch": 0.16,
      "grad_norm": 129.8555145263672,
      "learning_rate": 6.363636363636364e-06,
      "loss": 1.5401,
      "step": 225
    },
    {
      "epoch": 0.16355555555555557,
      "grad_norm": 53.34931182861328,
      "learning_rate": 6.505681818181818e-06,
      "loss": 1.342,
      "step": 230
    },
    {
      "epoch": 0.1671111111111111,
      "grad_norm": 57.25017547607422,
      "learning_rate": 6.647727272727273e-06,
      "loss": 1.272,
      "step": 235
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 65.65167999267578,
      "learning_rate": 6.789772727272727e-06,
      "loss": 1.4135,
      "step": 240
    },
    {
      "epoch": 0.17422222222222222,
      "grad_norm": 86.63700866699219,
      "learning_rate": 6.931818181818183e-06,
      "loss": 1.0604,
      "step": 245
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 43.24480438232422,
      "learning_rate": 7.073863636363637e-06,
      "loss": 1.2321,
      "step": 250
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 37.08866882324219,
      "learning_rate": 7.215909090909091e-06,
      "loss": 1.1643,
      "step": 255
    },
    {
      "epoch": 0.18488888888888888,
      "grad_norm": 75.33220672607422,
      "learning_rate": 7.357954545454546e-06,
      "loss": 0.7886,
      "step": 260
    },
    {
      "epoch": 0.18844444444444444,
      "grad_norm": 58.181983947753906,
      "learning_rate": 7.500000000000001e-06,
      "loss": 0.7873,
      "step": 265
    },
    {
      "epoch": 0.192,
      "grad_norm": 56.72947311401367,
      "learning_rate": 7.642045454545454e-06,
      "loss": 0.9271,
      "step": 270
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 47.29176330566406,
      "learning_rate": 7.784090909090911e-06,
      "loss": 0.7003,
      "step": 275
    },
    {
      "epoch": 0.1991111111111111,
      "grad_norm": 49.875877380371094,
      "learning_rate": 7.926136363636364e-06,
      "loss": 0.7117,
      "step": 280
    },
    {
      "epoch": 0.1991111111111111,
      "eval_accuracy": 0.7772,
      "eval_f1": 0.7858515955401768,
      "eval_loss": 0.8535242080688477,
      "eval_precision": 0.7564766839378239,
      "eval_recall": 0.8176,
      "eval_roc_auc": 0.87085032,
      "eval_runtime": 704.8651,
      "eval_samples_per_second": 7.094,
      "eval_steps_per_second": 0.444,
      "step": 280
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 111.80158996582031,
      "learning_rate": 8.068181818181819e-06,
      "loss": 0.7958,
      "step": 285
    },
    {
      "epoch": 0.20622222222222222,
      "grad_norm": 94.6966552734375,
      "learning_rate": 8.210227272727274e-06,
      "loss": 0.8627,
      "step": 290
    },
    {
      "epoch": 0.20977777777777779,
      "grad_norm": 59.3775520324707,
      "learning_rate": 8.352272727272727e-06,
      "loss": 0.8022,
      "step": 295
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 76.68104553222656,
      "learning_rate": 8.494318181818184e-06,
      "loss": 0.8171,
      "step": 300
    },
    {
      "epoch": 0.21688888888888888,
      "grad_norm": 85.33419799804688,
      "learning_rate": 8.636363636363637e-06,
      "loss": 0.651,
      "step": 305
    },
    {
      "epoch": 0.22044444444444444,
      "grad_norm": 53.72904586791992,
      "learning_rate": 8.778409090909092e-06,
      "loss": 0.4569,
      "step": 310
    },
    {
      "epoch": 0.224,
      "grad_norm": 30.077117919921875,
      "learning_rate": 8.920454545454547e-06,
      "loss": 0.6054,
      "step": 315
    },
    {
      "epoch": 0.22755555555555557,
      "grad_norm": 45.95614242553711,
      "learning_rate": 9.0625e-06,
      "loss": 0.662,
      "step": 320
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 31.222715377807617,
      "learning_rate": 9.204545454545455e-06,
      "loss": 0.4518,
      "step": 325
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 52.73350143432617,
      "learning_rate": 9.34659090909091e-06,
      "loss": 0.4396,
      "step": 330
    },
    {
      "epoch": 0.23822222222222222,
      "grad_norm": 25.8976993560791,
      "learning_rate": 9.488636363636365e-06,
      "loss": 0.3597,
      "step": 335
    },
    {
      "epoch": 0.24177777777777779,
      "grad_norm": 32.228023529052734,
      "learning_rate": 9.630681818181818e-06,
      "loss": 0.5253,
      "step": 340
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 66.55079650878906,
      "learning_rate": 9.772727272727273e-06,
      "loss": 0.5565,
      "step": 345
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 69.90823364257812,
      "learning_rate": 9.914772727272728e-06,
      "loss": 0.4917,
      "step": 350
    },
    {
      "epoch": 0.25244444444444447,
      "grad_norm": 48.786170959472656,
      "learning_rate": 1.0056818181818183e-05,
      "loss": 0.5144,
      "step": 355
    },
    {
      "epoch": 0.256,
      "grad_norm": 72.33039855957031,
      "learning_rate": 1.0198863636363636e-05,
      "loss": 0.4311,
      "step": 360
    },
    {
      "epoch": 0.25955555555555554,
      "grad_norm": 20.26807403564453,
      "learning_rate": 1.0340909090909093e-05,
      "loss": 0.1876,
      "step": 365
    },
    {
      "epoch": 0.26311111111111113,
      "grad_norm": 57.392391204833984,
      "learning_rate": 1.0482954545454548e-05,
      "loss": 0.586,
      "step": 370
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 38.75966262817383,
      "learning_rate": 1.0625e-05,
      "loss": 0.4419,
      "step": 375
    },
    {
      "epoch": 0.2702222222222222,
      "grad_norm": 53.099761962890625,
      "learning_rate": 1.0767045454545456e-05,
      "loss": 0.477,
      "step": 380
    },
    {
      "epoch": 0.2737777777777778,
      "grad_norm": 40.86305236816406,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 0.3384,
      "step": 385
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 44.48622131347656,
      "learning_rate": 1.1051136363636366e-05,
      "loss": 0.5449,
      "step": 390
    },
    {
      "epoch": 0.2808888888888889,
      "grad_norm": 66.54920959472656,
      "learning_rate": 1.119318181818182e-05,
      "loss": 0.5118,
      "step": 395
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 61.06609344482422,
      "learning_rate": 1.1335227272727274e-05,
      "loss": 0.3217,
      "step": 400
    },
    {
      "epoch": 0.288,
      "grad_norm": 33.951324462890625,
      "learning_rate": 1.1477272727272729e-05,
      "loss": 0.2335,
      "step": 405
    },
    {
      "epoch": 0.29155555555555557,
      "grad_norm": 32.54682922363281,
      "learning_rate": 1.1619318181818182e-05,
      "loss": 0.4336,
      "step": 410
    },
    {
      "epoch": 0.2951111111111111,
      "grad_norm": 42.70922088623047,
      "learning_rate": 1.1761363636363637e-05,
      "loss": 0.4446,
      "step": 415
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 92.10637664794922,
      "learning_rate": 1.1903409090909093e-05,
      "loss": 0.3515,
      "step": 420
    },
    {
      "epoch": 0.2986666666666667,
      "eval_accuracy": 0.893,
      "eval_f1": 0.893234883256835,
      "eval_loss": 0.38197317719459534,
      "eval_precision": 0.8912783751493429,
      "eval_recall": 0.8952,
      "eval_roc_auc": 0.9629016800000001,
      "eval_runtime": 705.9853,
      "eval_samples_per_second": 7.082,
      "eval_steps_per_second": 0.443,
      "step": 420
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 27.56109619140625,
      "learning_rate": 1.2045454545454547e-05,
      "loss": 0.3172,
      "step": 425
    },
    {
      "epoch": 0.30577777777777776,
      "grad_norm": 55.959964752197266,
      "learning_rate": 1.2187500000000001e-05,
      "loss": 0.3282,
      "step": 430
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 49.05080795288086,
      "learning_rate": 1.2329545454545455e-05,
      "loss": 0.1617,
      "step": 435
    },
    {
      "epoch": 0.3128888888888889,
      "grad_norm": 98.04788208007812,
      "learning_rate": 1.247159090909091e-05,
      "loss": 0.1758,
      "step": 440
    },
    {
      "epoch": 0.3164444444444444,
      "grad_norm": 34.51274108886719,
      "learning_rate": 1.2613636363636366e-05,
      "loss": 0.314,
      "step": 445
    },
    {
      "epoch": 0.32,
      "grad_norm": 29.37026596069336,
      "learning_rate": 1.275568181818182e-05,
      "loss": 0.2246,
      "step": 450
    },
    {
      "epoch": 0.32355555555555554,
      "grad_norm": 52.17829895019531,
      "learning_rate": 1.2897727272727274e-05,
      "loss": 0.4854,
      "step": 455
    },
    {
      "epoch": 0.32711111111111113,
      "grad_norm": 27.550739288330078,
      "learning_rate": 1.3039772727272728e-05,
      "loss": 0.3832,
      "step": 460
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 59.2094841003418,
      "learning_rate": 1.3181818181818183e-05,
      "loss": 0.4539,
      "step": 465
    },
    {
      "epoch": 0.3342222222222222,
      "grad_norm": 67.14048767089844,
      "learning_rate": 1.3323863636363636e-05,
      "loss": 0.495,
      "step": 470
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 17.25642967224121,
      "learning_rate": 1.3465909090909092e-05,
      "loss": 0.3297,
      "step": 475
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 10.905756950378418,
      "learning_rate": 1.3607954545454547e-05,
      "loss": 0.4248,
      "step": 480
    },
    {
      "epoch": 0.3448888888888889,
      "grad_norm": 30.15828514099121,
      "learning_rate": 1.375e-05,
      "loss": 0.3879,
      "step": 485
    },
    {
      "epoch": 0.34844444444444445,
      "grad_norm": 101.98735809326172,
      "learning_rate": 1.3892045454545455e-05,
      "loss": 0.4154,
      "step": 490
    },
    {
      "epoch": 0.352,
      "grad_norm": 26.732345581054688,
      "learning_rate": 1.4034090909090909e-05,
      "loss": 0.2209,
      "step": 495
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 165.9346160888672,
      "learning_rate": 1.4176136363636365e-05,
      "loss": 0.3601,
      "step": 500
    },
    {
      "epoch": 0.3591111111111111,
      "grad_norm": 137.78173828125,
      "learning_rate": 1.431818181818182e-05,
      "loss": 0.4664,
      "step": 505
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 25.728025436401367,
      "learning_rate": 1.4460227272727273e-05,
      "loss": 0.4216,
      "step": 510
    },
    {
      "epoch": 0.3662222222222222,
      "grad_norm": 42.744319915771484,
      "learning_rate": 1.4602272727272728e-05,
      "loss": 0.2651,
      "step": 515
    },
    {
      "epoch": 0.36977777777777776,
      "grad_norm": 61.11492919921875,
      "learning_rate": 1.4744318181818183e-05,
      "loss": 0.4605,
      "step": 520
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 38.179683685302734,
      "learning_rate": 1.4886363636363636e-05,
      "loss": 0.2093,
      "step": 525
    },
    {
      "epoch": 0.3768888888888889,
      "grad_norm": 42.08168029785156,
      "learning_rate": 1.5028409090909093e-05,
      "loss": 0.324,
      "step": 530
    },
    {
      "epoch": 0.3804444444444444,
      "grad_norm": 78.061279296875,
      "learning_rate": 1.5170454545454546e-05,
      "loss": 0.3367,
      "step": 535
    },
    {
      "epoch": 0.384,
      "grad_norm": 47.140968322753906,
      "learning_rate": 1.5312500000000003e-05,
      "loss": 0.3745,
      "step": 540
    },
    {
      "epoch": 0.38755555555555554,
      "grad_norm": 59.479103088378906,
      "learning_rate": 1.5454545454545454e-05,
      "loss": 0.2926,
      "step": 545
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 27.827219009399414,
      "learning_rate": 1.559659090909091e-05,
      "loss": 0.2778,
      "step": 550
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 18.94660186767578,
      "learning_rate": 1.5738636363636364e-05,
      "loss": 0.112,
      "step": 555
    },
    {
      "epoch": 0.3982222222222222,
      "grad_norm": 16.1043758392334,
      "learning_rate": 1.588068181818182e-05,
      "loss": 0.401,
      "step": 560
    },
    {
      "epoch": 0.3982222222222222,
      "eval_accuracy": 0.8654,
      "eval_f1": 0.8479783148859272,
      "eval_loss": 0.546054482460022,
      "eval_precision": 0.9740529320186819,
      "eval_recall": 0.7508,
      "eval_roc_auc": 0.96116112,
      "eval_runtime": 738.9805,
      "eval_samples_per_second": 6.766,
      "eval_steps_per_second": 0.424,
      "step": 560
    },
    {
      "epoch": 0.4017777777777778,
      "grad_norm": 68.98033142089844,
      "learning_rate": 1.6022727272727274e-05,
      "loss": 0.5888,
      "step": 565
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 46.15235137939453,
      "learning_rate": 1.616477272727273e-05,
      "loss": 0.1392,
      "step": 570
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 82.76618194580078,
      "learning_rate": 1.6306818181818184e-05,
      "loss": 0.4366,
      "step": 575
    },
    {
      "epoch": 0.41244444444444445,
      "grad_norm": 15.22512435913086,
      "learning_rate": 1.6448863636363635e-05,
      "loss": 0.1262,
      "step": 580
    },
    {
      "epoch": 0.416,
      "grad_norm": 67.41675567626953,
      "learning_rate": 1.6590909090909094e-05,
      "loss": 0.2615,
      "step": 585
    },
    {
      "epoch": 0.41955555555555557,
      "grad_norm": 56.8907585144043,
      "learning_rate": 1.673295454545455e-05,
      "loss": 0.2677,
      "step": 590
    },
    {
      "epoch": 0.4231111111111111,
      "grad_norm": 34.207252502441406,
      "learning_rate": 1.6875e-05,
      "loss": 0.1669,
      "step": 595
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 29.28672981262207,
      "learning_rate": 1.7017045454545455e-05,
      "loss": 0.442,
      "step": 600
    },
    {
      "epoch": 0.43022222222222223,
      "grad_norm": 30.626209259033203,
      "learning_rate": 1.715909090909091e-05,
      "loss": 0.2128,
      "step": 605
    },
    {
      "epoch": 0.43377777777777776,
      "grad_norm": 63.92142868041992,
      "learning_rate": 1.7301136363636365e-05,
      "loss": 0.2073,
      "step": 610
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 9.031936645507812,
      "learning_rate": 1.744318181818182e-05,
      "loss": 0.1826,
      "step": 615
    },
    {
      "epoch": 0.4408888888888889,
      "grad_norm": 69.81771087646484,
      "learning_rate": 1.7585227272727275e-05,
      "loss": 0.2381,
      "step": 620
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 29.49026107788086,
      "learning_rate": 1.772727272727273e-05,
      "loss": 0.259,
      "step": 625
    },
    {
      "epoch": 0.448,
      "grad_norm": 46.705711364746094,
      "learning_rate": 1.786931818181818e-05,
      "loss": 0.0974,
      "step": 630
    },
    {
      "epoch": 0.45155555555555554,
      "grad_norm": 47.539939880371094,
      "learning_rate": 1.8011363636363636e-05,
      "loss": 0.1264,
      "step": 635
    },
    {
      "epoch": 0.45511111111111113,
      "grad_norm": 41.530303955078125,
      "learning_rate": 1.8153409090909094e-05,
      "loss": 0.2351,
      "step": 640
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 63.43205261230469,
      "learning_rate": 1.8295454545454546e-05,
      "loss": 0.1599,
      "step": 645
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 19.165851593017578,
      "learning_rate": 1.84375e-05,
      "loss": 0.0688,
      "step": 650
    },
    {
      "epoch": 0.4657777777777778,
      "grad_norm": 9.95614242553711,
      "learning_rate": 1.8579545454545456e-05,
      "loss": 0.093,
      "step": 655
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 37.27664566040039,
      "learning_rate": 1.872159090909091e-05,
      "loss": 0.1863,
      "step": 660
    },
    {
      "epoch": 0.4728888888888889,
      "grad_norm": 5.4267258644104,
      "learning_rate": 1.8863636363636366e-05,
      "loss": 0.1569,
      "step": 665
    },
    {
      "epoch": 0.47644444444444445,
      "grad_norm": 25.495676040649414,
      "learning_rate": 1.900568181818182e-05,
      "loss": 0.1297,
      "step": 670
    },
    {
      "epoch": 0.48,
      "grad_norm": 18.486164093017578,
      "learning_rate": 1.9147727272727276e-05,
      "loss": 0.1719,
      "step": 675
    },
    {
      "epoch": 0.48355555555555557,
      "grad_norm": 5.234048366546631,
      "learning_rate": 1.9289772727272727e-05,
      "loss": 0.0575,
      "step": 680
    },
    {
      "epoch": 0.4871111111111111,
      "grad_norm": 33.791629791259766,
      "learning_rate": 1.9431818181818182e-05,
      "loss": 0.2091,
      "step": 685
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 20.584003448486328,
      "learning_rate": 1.9573863636363637e-05,
      "loss": 0.1953,
      "step": 690
    },
    {
      "epoch": 0.49422222222222223,
      "grad_norm": 29.12327766418457,
      "learning_rate": 1.9715909090909092e-05,
      "loss": 0.2721,
      "step": 695
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 2.4427449703216553,
      "learning_rate": 1.9857954545454547e-05,
      "loss": 0.2211,
      "step": 700
    },
    {
      "epoch": 0.49777777777777776,
      "eval_accuracy": 0.9452,
      "eval_f1": 0.94515612489992,
      "eval_loss": 0.16872349381446838,
      "eval_precision": 0.9459134615384616,
      "eval_recall": 0.9444,
      "eval_roc_auc": 0.98587952,
      "eval_runtime": 705.2695,
      "eval_samples_per_second": 7.089,
      "eval_steps_per_second": 0.444,
      "step": 700
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 27.946657180786133,
      "learning_rate": 2e-05,
      "loss": 0.2363,
      "step": 705
    },
    {
      "epoch": 0.5048888888888889,
      "grad_norm": 12.571545600891113,
      "learning_rate": 1.9999969220262722e-05,
      "loss": 0.1829,
      "step": 710
    },
    {
      "epoch": 0.5084444444444445,
      "grad_norm": 25.812358856201172,
      "learning_rate": 1.999987688124037e-05,
      "loss": 0.0959,
      "step": 715
    },
    {
      "epoch": 0.512,
      "grad_norm": 17.006311416625977,
      "learning_rate": 1.9999722983501374e-05,
      "loss": 0.1094,
      "step": 720
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 34.0295524597168,
      "learning_rate": 1.9999507527993122e-05,
      "loss": 0.1692,
      "step": 725
    },
    {
      "epoch": 0.5191111111111111,
      "grad_norm": 42.78932189941406,
      "learning_rate": 1.9999230516041947e-05,
      "loss": 0.0893,
      "step": 730
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 14.910472869873047,
      "learning_rate": 1.9998891949353116e-05,
      "loss": 0.218,
      "step": 735
    },
    {
      "epoch": 0.5262222222222223,
      "grad_norm": 56.67222213745117,
      "learning_rate": 1.9998491830010835e-05,
      "loss": 0.085,
      "step": 740
    },
    {
      "epoch": 0.5297777777777778,
      "grad_norm": 44.31144714355469,
      "learning_rate": 1.9998030160478214e-05,
      "loss": 0.1856,
      "step": 745
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 38.34380340576172,
      "learning_rate": 1.9997506943597264e-05,
      "loss": 0.1665,
      "step": 750
    },
    {
      "epoch": 0.5368888888888889,
      "grad_norm": 50.9335823059082,
      "learning_rate": 1.9996922182588887e-05,
      "loss": 0.1351,
      "step": 755
    },
    {
      "epoch": 0.5404444444444444,
      "grad_norm": 36.42063522338867,
      "learning_rate": 1.9996275881052835e-05,
      "loss": 0.1978,
      "step": 760
    },
    {
      "epoch": 0.544,
      "grad_norm": 18.1893367767334,
      "learning_rate": 1.999556804296771e-05,
      "loss": 0.2936,
      "step": 765
    },
    {
      "epoch": 0.5475555555555556,
      "grad_norm": 84.25397491455078,
      "learning_rate": 1.9994798672690922e-05,
      "loss": 0.2763,
      "step": 770
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 39.89824676513672,
      "learning_rate": 1.9993967774958682e-05,
      "loss": 0.3607,
      "step": 775
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 28.9774169921875,
      "learning_rate": 1.999307535488594e-05,
      "loss": 0.1402,
      "step": 780
    },
    {
      "epoch": 0.5582222222222222,
      "grad_norm": 57.86973190307617,
      "learning_rate": 1.99921214179664e-05,
      "loss": 0.1757,
      "step": 785
    },
    {
      "epoch": 0.5617777777777778,
      "grad_norm": 78.73992156982422,
      "learning_rate": 1.999110597007244e-05,
      "loss": 0.436,
      "step": 790
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 116.64632415771484,
      "learning_rate": 1.9990029017455102e-05,
      "loss": 0.2069,
      "step": 795
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 74.38374328613281,
      "learning_rate": 1.998889056674406e-05,
      "loss": 0.2554,
      "step": 800
    },
    {
      "epoch": 0.5724444444444444,
      "grad_norm": 2.4084792137145996,
      "learning_rate": 1.998769062494755e-05,
      "loss": 0.2354,
      "step": 805
    },
    {
      "epoch": 0.576,
      "grad_norm": 70.7164535522461,
      "learning_rate": 1.9986429199452347e-05,
      "loss": 0.2933,
      "step": 810
    },
    {
      "epoch": 0.5795555555555556,
      "grad_norm": 8.50871467590332,
      "learning_rate": 1.9985106298023727e-05,
      "loss": 0.1319,
      "step": 815
    },
    {
      "epoch": 0.5831111111111111,
      "grad_norm": 8.591714859008789,
      "learning_rate": 1.9983721928805397e-05,
      "loss": 0.1519,
      "step": 820
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 24.299182891845703,
      "learning_rate": 1.9982276100319463e-05,
      "loss": 0.2185,
      "step": 825
    },
    {
      "epoch": 0.5902222222222222,
      "grad_norm": 89.42211151123047,
      "learning_rate": 1.998076882146637e-05,
      "loss": 0.1751,
      "step": 830
    },
    {
      "epoch": 0.5937777777777777,
      "grad_norm": 7.560657024383545,
      "learning_rate": 1.9979200101524844e-05,
      "loss": 0.0923,
      "step": 835
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 41.54198455810547,
      "learning_rate": 1.9977569950151848e-05,
      "loss": 0.1382,
      "step": 840
    },
    {
      "epoch": 0.5973333333333334,
      "eval_accuracy": 0.9606,
      "eval_f1": 0.9605447626677348,
      "eval_loss": 0.1394517719745636,
      "eval_precision": 0.9618933012434817,
      "eval_recall": 0.9592,
      "eval_roc_auc": 0.9913924000000001,
      "eval_runtime": 698.639,
      "eval_samples_per_second": 7.157,
      "eval_steps_per_second": 0.448,
      "step": 840
    },
    {
      "epoch": 0.6008888888888889,
      "grad_norm": 63.37054443359375,
      "learning_rate": 1.9975878377382505e-05,
      "loss": 0.077,
      "step": 845
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 19.045230865478516,
      "learning_rate": 1.997412539363005e-05,
      "loss": 0.0787,
      "step": 850
    },
    {
      "epoch": 0.608,
      "grad_norm": 17.874940872192383,
      "learning_rate": 1.9972311009685753e-05,
      "loss": 0.1182,
      "step": 855
    },
    {
      "epoch": 0.6115555555555555,
      "grad_norm": 34.53679275512695,
      "learning_rate": 1.997043523671887e-05,
      "loss": 0.2478,
      "step": 860
    },
    {
      "epoch": 0.6151111111111112,
      "grad_norm": 27.373130798339844,
      "learning_rate": 1.9968498086276565e-05,
      "loss": 0.277,
      "step": 865
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 3.571431875228882,
      "learning_rate": 1.996649957028383e-05,
      "loss": 0.0758,
      "step": 870
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 2.8354272842407227,
      "learning_rate": 1.9964439701043422e-05,
      "loss": 0.0898,
      "step": 875
    },
    {
      "epoch": 0.6257777777777778,
      "grad_norm": 50.368377685546875,
      "learning_rate": 1.9962318491235795e-05,
      "loss": 0.1844,
      "step": 880
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 5.133639335632324,
      "learning_rate": 1.9960135953918996e-05,
      "loss": 0.0817,
      "step": 885
    },
    {
      "epoch": 0.6328888888888888,
      "grad_norm": 55.377681732177734,
      "learning_rate": 1.995789210252862e-05,
      "loss": 0.1277,
      "step": 890
    },
    {
      "epoch": 0.6364444444444445,
      "grad_norm": 28.40950584411621,
      "learning_rate": 1.9955586950877693e-05,
      "loss": 0.139,
      "step": 895
    },
    {
      "epoch": 0.64,
      "grad_norm": 43.362953186035156,
      "learning_rate": 1.9953220513156604e-05,
      "loss": 0.0512,
      "step": 900
    },
    {
      "epoch": 0.6435555555555555,
      "grad_norm": 28.17371368408203,
      "learning_rate": 1.9950792803933024e-05,
      "loss": 0.0811,
      "step": 905
    },
    {
      "epoch": 0.6471111111111111,
      "grad_norm": 24.614852905273438,
      "learning_rate": 1.9948303838151805e-05,
      "loss": 0.1533,
      "step": 910
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 20.735342025756836,
      "learning_rate": 1.9945753631134884e-05,
      "loss": 0.1884,
      "step": 915
    },
    {
      "epoch": 0.6542222222222223,
      "grad_norm": 55.86494445800781,
      "learning_rate": 1.9943142198581203e-05,
      "loss": 0.1591,
      "step": 920
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 66.37036895751953,
      "learning_rate": 1.9940469556566612e-05,
      "loss": 0.1171,
      "step": 925
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.1971541792154312,
      "learning_rate": 1.9937735721543742e-05,
      "loss": 0.1122,
      "step": 930
    },
    {
      "epoch": 0.6648888888888889,
      "grad_norm": 7.523128986358643,
      "learning_rate": 1.9934940710341942e-05,
      "loss": 0.0718,
      "step": 935
    },
    {
      "epoch": 0.6684444444444444,
      "grad_norm": 38.67007064819336,
      "learning_rate": 1.993208454016716e-05,
      "loss": 0.1428,
      "step": 940
    },
    {
      "epoch": 0.672,
      "grad_norm": 1.3185166120529175,
      "learning_rate": 1.992916722860182e-05,
      "loss": 0.0257,
      "step": 945
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 31.755578994750977,
      "learning_rate": 1.992618879360475e-05,
      "loss": 0.0754,
      "step": 950
    },
    {
      "epoch": 0.6791111111111111,
      "grad_norm": 74.46941375732422,
      "learning_rate": 1.9923149253511028e-05,
      "loss": 0.2749,
      "step": 955
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 47.32447814941406,
      "learning_rate": 1.9920048627031915e-05,
      "loss": 0.0922,
      "step": 960
    },
    {
      "epoch": 0.6862222222222222,
      "grad_norm": 28.85693359375,
      "learning_rate": 1.991688693325469e-05,
      "loss": 0.1127,
      "step": 965
    },
    {
      "epoch": 0.6897777777777778,
      "grad_norm": 89.10200500488281,
      "learning_rate": 1.9913664191642587e-05,
      "loss": 0.3633,
      "step": 970
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 8.073657035827637,
      "learning_rate": 1.9910380422034627e-05,
      "loss": 0.0629,
      "step": 975
    },
    {
      "epoch": 0.6968888888888889,
      "grad_norm": 18.657575607299805,
      "learning_rate": 1.9907035644645524e-05,
      "loss": 0.0869,
      "step": 980
    },
    {
      "epoch": 0.6968888888888889,
      "eval_accuracy": 0.9534,
      "eval_f1": 0.9515290201789057,
      "eval_loss": 0.18388661742210388,
      "eval_precision": 0.9913307325530992,
      "eval_recall": 0.9148,
      "eval_roc_auc": 0.99242576,
      "eval_runtime": 718.1569,
      "eval_samples_per_second": 6.962,
      "eval_steps_per_second": 0.436,
      "step": 980
    },
    {
      "epoch": 0.7004444444444444,
      "grad_norm": 13.230423927307129,
      "learning_rate": 1.9903629880065555e-05,
      "loss": 0.1517,
      "step": 985
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.8867300748825073,
      "learning_rate": 1.9900163149260424e-05,
      "loss": 0.0617,
      "step": 990
    },
    {
      "epoch": 0.7075555555555556,
      "grad_norm": 10.52268123626709,
      "learning_rate": 1.9896635473571145e-05,
      "loss": 0.0517,
      "step": 995
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 20.254636764526367,
      "learning_rate": 1.9893046874713903e-05,
      "loss": 0.0401,
      "step": 1000
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 20.974159240722656,
      "learning_rate": 1.9889397374779927e-05,
      "loss": 0.1028,
      "step": 1005
    },
    {
      "epoch": 0.7182222222222222,
      "grad_norm": 5.5051655769348145,
      "learning_rate": 1.9885686996235348e-05,
      "loss": 0.0736,
      "step": 1010
    },
    {
      "epoch": 0.7217777777777777,
      "grad_norm": 55.200401306152344,
      "learning_rate": 1.9881915761921052e-05,
      "loss": 0.2333,
      "step": 1015
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 7.092331409454346,
      "learning_rate": 1.9878083695052572e-05,
      "loss": 0.1645,
      "step": 1020
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 100.41136932373047,
      "learning_rate": 1.98741908192199e-05,
      "loss": 0.117,
      "step": 1025
    },
    {
      "epoch": 0.7324444444444445,
      "grad_norm": 5.539927959442139,
      "learning_rate": 1.9870237158387385e-05,
      "loss": 0.0941,
      "step": 1030
    },
    {
      "epoch": 0.736,
      "grad_norm": 12.838068962097168,
      "learning_rate": 1.9866222736893544e-05,
      "loss": 0.16,
      "step": 1035
    },
    {
      "epoch": 0.7395555555555555,
      "grad_norm": 62.58792495727539,
      "learning_rate": 1.9862147579450953e-05,
      "loss": 0.1405,
      "step": 1040
    },
    {
      "epoch": 0.7431111111111111,
      "grad_norm": 6.154151439666748,
      "learning_rate": 1.9858011711146062e-05,
      "loss": 0.1694,
      "step": 1045
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 81.01847076416016,
      "learning_rate": 1.9853815157439065e-05,
      "loss": 0.1845,
      "step": 1050
    },
    {
      "epoch": 0.7502222222222222,
      "grad_norm": 45.606441497802734,
      "learning_rate": 1.9849557944163723e-05,
      "loss": 0.1939,
      "step": 1055
    },
    {
      "epoch": 0.7537777777777778,
      "grad_norm": 14.879412651062012,
      "learning_rate": 1.984524009752721e-05,
      "loss": 0.0916,
      "step": 1060
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 37.300418853759766,
      "learning_rate": 1.9840861644109977e-05,
      "loss": 0.0959,
      "step": 1065
    },
    {
      "epoch": 0.7608888888888888,
      "grad_norm": 24.60807228088379,
      "learning_rate": 1.9836422610865544e-05,
      "loss": 0.3176,
      "step": 1070
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 85.29894256591797,
      "learning_rate": 1.9831923025120367e-05,
      "loss": 0.1845,
      "step": 1075
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.16374729573726654,
      "learning_rate": 1.982736291457366e-05,
      "loss": 0.057,
      "step": 1080
    },
    {
      "epoch": 0.7715555555555556,
      "grad_norm": 16.288103103637695,
      "learning_rate": 1.982274230729723e-05,
      "loss": 0.1061,
      "step": 1085
    },
    {
      "epoch": 0.7751111111111111,
      "grad_norm": 38.656002044677734,
      "learning_rate": 1.981806123173528e-05,
      "loss": 0.1081,
      "step": 1090
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 25.296985626220703,
      "learning_rate": 1.9813319716704278e-05,
      "loss": 0.0814,
      "step": 1095
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 7.374482154846191,
      "learning_rate": 1.980851779139273e-05,
      "loss": 0.0285,
      "step": 1100
    },
    {
      "epoch": 0.7857777777777778,
      "grad_norm": 64.1263656616211,
      "learning_rate": 1.9803655485361048e-05,
      "loss": 0.1413,
      "step": 1105
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 27.25572395324707,
      "learning_rate": 1.9798732828541325e-05,
      "loss": 0.3252,
      "step": 1110
    },
    {
      "epoch": 0.7928888888888889,
      "grad_norm": 4.3475022315979,
      "learning_rate": 1.979374985123718e-05,
      "loss": 0.085,
      "step": 1115
    },
    {
      "epoch": 0.7964444444444444,
      "grad_norm": 22.632213592529297,
      "learning_rate": 1.9788706584123555e-05,
      "loss": 0.1236,
      "step": 1120
    },
    {
      "epoch": 0.7964444444444444,
      "eval_accuracy": 0.971,
      "eval_f1": 0.9706299372088313,
      "eval_loss": 0.09948600083589554,
      "eval_precision": 0.9831760361099713,
      "eval_recall": 0.9584,
      "eval_roc_auc": 0.9946903199999999,
      "eval_runtime": 732.1631,
      "eval_samples_per_second": 6.829,
      "eval_steps_per_second": 0.428,
      "step": 1120
    },
    {
      "epoch": 0.8,
      "grad_norm": 33.177913665771484,
      "learning_rate": 1.9783603058246542e-05,
      "loss": 0.3204,
      "step": 1125
    },
    {
      "epoch": 0.8035555555555556,
      "grad_norm": 29.69413185119629,
      "learning_rate": 1.9778439305023175e-05,
      "loss": 0.0929,
      "step": 1130
    },
    {
      "epoch": 0.8071111111111111,
      "grad_norm": 65.44840240478516,
      "learning_rate": 1.9773215356241255e-05,
      "loss": 0.1661,
      "step": 1135
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 4.2236199378967285,
      "learning_rate": 1.9767931244059126e-05,
      "loss": 0.0986,
      "step": 1140
    },
    {
      "epoch": 0.8142222222222222,
      "grad_norm": 46.74170684814453,
      "learning_rate": 1.976258700100551e-05,
      "loss": 0.1186,
      "step": 1145
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 17.90570640563965,
      "learning_rate": 1.975718265997929e-05,
      "loss": 0.101,
      "step": 1150
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 91.6234359741211,
      "learning_rate": 1.9751718254249294e-05,
      "loss": 0.1866,
      "step": 1155
    },
    {
      "epoch": 0.8248888888888889,
      "grad_norm": 53.775108337402344,
      "learning_rate": 1.9746193817454128e-05,
      "loss": 0.2089,
      "step": 1160
    },
    {
      "epoch": 0.8284444444444444,
      "grad_norm": 24.398826599121094,
      "learning_rate": 1.9740609383601933e-05,
      "loss": 0.0723,
      "step": 1165
    },
    {
      "epoch": 0.832,
      "grad_norm": 5.196507930755615,
      "learning_rate": 1.9734964987070185e-05,
      "loss": 0.1026,
      "step": 1170
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 21.41330337524414,
      "learning_rate": 1.9729260662605497e-05,
      "loss": 0.074,
      "step": 1175
    },
    {
      "epoch": 0.8391111111111111,
      "grad_norm": 1.0466164350509644,
      "learning_rate": 1.9723496445323385e-05,
      "loss": 0.007,
      "step": 1180
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 3.176678419113159,
      "learning_rate": 1.9717672370708075e-05,
      "loss": 0.0328,
      "step": 1185
    },
    {
      "epoch": 0.8462222222222222,
      "grad_norm": 70.44237518310547,
      "learning_rate": 1.9711788474612263e-05,
      "loss": 0.2665,
      "step": 1190
    },
    {
      "epoch": 0.8497777777777777,
      "grad_norm": 21.05078125,
      "learning_rate": 1.97058447932569e-05,
      "loss": 0.0599,
      "step": 1195
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 34.70158767700195,
      "learning_rate": 1.969984136323098e-05,
      "loss": 0.113,
      "step": 1200
    },
    {
      "epoch": 0.8568888888888889,
      "grad_norm": 42.98965072631836,
      "learning_rate": 1.96937782214913e-05,
      "loss": 0.0937,
      "step": 1205
    },
    {
      "epoch": 0.8604444444444445,
      "grad_norm": 60.753761291503906,
      "learning_rate": 1.968765540536224e-05,
      "loss": 0.1481,
      "step": 1210
    },
    {
      "epoch": 0.864,
      "grad_norm": 32.72594451904297,
      "learning_rate": 1.9681472952535544e-05,
      "loss": 0.125,
      "step": 1215
    },
    {
      "epoch": 0.8675555555555555,
      "grad_norm": 12.429193496704102,
      "learning_rate": 1.9675230901070058e-05,
      "loss": 0.04,
      "step": 1220
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 17.702072143554688,
      "learning_rate": 1.9668929289391524e-05,
      "loss": 0.1226,
      "step": 1225
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 66.32637023925781,
      "learning_rate": 1.9662568156292334e-05,
      "loss": 0.1408,
      "step": 1230
    },
    {
      "epoch": 0.8782222222222222,
      "grad_norm": 22.44866943359375,
      "learning_rate": 1.9656147540931288e-05,
      "loss": 0.15,
      "step": 1235
    },
    {
      "epoch": 0.8817777777777778,
      "grad_norm": 20.30470085144043,
      "learning_rate": 1.9649667482833356e-05,
      "loss": 0.0318,
      "step": 1240
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.8308598399162292,
      "learning_rate": 1.964312802188944e-05,
      "loss": 0.1035,
      "step": 1245
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 16.766145706176758,
      "learning_rate": 1.9636529198356112e-05,
      "loss": 0.0975,
      "step": 1250
    },
    {
      "epoch": 0.8924444444444445,
      "grad_norm": 18.3680362701416,
      "learning_rate": 1.962987105285539e-05,
      "loss": 0.0623,
      "step": 1255
    },
    {
      "epoch": 0.896,
      "grad_norm": 13.790369033813477,
      "learning_rate": 1.9623153626374458e-05,
      "loss": 0.1009,
      "step": 1260
    },
    {
      "epoch": 0.896,
      "eval_accuracy": 0.9644,
      "eval_f1": 0.9632231404958679,
      "eval_loss": 0.14218686521053314,
      "eval_precision": 0.9961538461538462,
      "eval_recall": 0.9324,
      "eval_roc_auc": 0.9941917600000001,
      "eval_runtime": 736.5739,
      "eval_samples_per_second": 6.788,
      "eval_steps_per_second": 0.425,
      "step": 1260
    },
    {
      "epoch": 0.8995555555555556,
      "grad_norm": 57.0408935546875,
      "learning_rate": 1.9616376960265445e-05,
      "loss": 0.1605,
      "step": 1265
    },
    {
      "epoch": 0.9031111111111111,
      "grad_norm": 25.690959930419922,
      "learning_rate": 1.9609541096245153e-05,
      "loss": 0.0959,
      "step": 1270
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 8.337908744812012,
      "learning_rate": 1.9602646076394806e-05,
      "loss": 0.056,
      "step": 1275
    },
    {
      "epoch": 0.9102222222222223,
      "grad_norm": 24.496253967285156,
      "learning_rate": 1.9595691943159777e-05,
      "loss": 0.0888,
      "step": 1280
    },
    {
      "epoch": 0.9137777777777778,
      "grad_norm": 36.771305084228516,
      "learning_rate": 1.9588678739349345e-05,
      "loss": 0.178,
      "step": 1285
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.5869746208190918,
      "learning_rate": 1.9581606508136426e-05,
      "loss": 0.1848,
      "step": 1290
    },
    {
      "epoch": 0.9208888888888889,
      "grad_norm": 2.8221828937530518,
      "learning_rate": 1.95744752930573e-05,
      "loss": 0.0347,
      "step": 1295
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 6.394200801849365,
      "learning_rate": 1.9567285138011365e-05,
      "loss": 0.1173,
      "step": 1300
    },
    {
      "epoch": 0.928,
      "grad_norm": 8.24280834197998,
      "learning_rate": 1.956003608726082e-05,
      "loss": 0.1735,
      "step": 1305
    },
    {
      "epoch": 0.9315555555555556,
      "grad_norm": 2.5479698181152344,
      "learning_rate": 1.9552728185430454e-05,
      "loss": 0.0621,
      "step": 1310
    },
    {
      "epoch": 0.9351111111111111,
      "grad_norm": 24.549339294433594,
      "learning_rate": 1.954536147750732e-05,
      "loss": 0.0504,
      "step": 1315
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 32.2918586730957,
      "learning_rate": 1.953793600884049e-05,
      "loss": 0.0336,
      "step": 1320
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 0.8282171487808228,
      "learning_rate": 1.9530451825140752e-05,
      "loss": 0.0384,
      "step": 1325
    },
    {
      "epoch": 0.9457777777777778,
      "grad_norm": 6.600639820098877,
      "learning_rate": 1.952290897248035e-05,
      "loss": 0.1273,
      "step": 1330
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 20.863340377807617,
      "learning_rate": 1.951530749729269e-05,
      "loss": 0.0689,
      "step": 1335
    },
    {
      "epoch": 0.9528888888888889,
      "grad_norm": 48.49995040893555,
      "learning_rate": 1.9507647446372056e-05,
      "loss": 0.1881,
      "step": 1340
    },
    {
      "epoch": 0.9564444444444444,
      "grad_norm": 10.825556755065918,
      "learning_rate": 1.9499928866873318e-05,
      "loss": 0.0532,
      "step": 1345
    },
    {
      "epoch": 0.96,
      "grad_norm": 20.550548553466797,
      "learning_rate": 1.949215180631164e-05,
      "loss": 0.1419,
      "step": 1350
    },
    {
      "epoch": 0.9635555555555556,
      "grad_norm": 16.873754501342773,
      "learning_rate": 1.9484316312562204e-05,
      "loss": 0.1305,
      "step": 1355
    },
    {
      "epoch": 0.9671111111111111,
      "grad_norm": 46.64189910888672,
      "learning_rate": 1.94764224338599e-05,
      "loss": 0.1152,
      "step": 1360
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 32.25148010253906,
      "learning_rate": 1.9468470218799025e-05,
      "loss": 0.0472,
      "step": 1365
    },
    {
      "epoch": 0.9742222222222222,
      "grad_norm": 0.49956056475639343,
      "learning_rate": 1.9460459716333e-05,
      "loss": 0.0555,
      "step": 1370
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 37.25569152832031,
      "learning_rate": 1.9452390975774054e-05,
      "loss": 0.1326,
      "step": 1375
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 2.0712389945983887,
      "learning_rate": 1.9444264046792934e-05,
      "loss": 0.0976,
      "step": 1380
    },
    {
      "epoch": 0.9848888888888889,
      "grad_norm": 11.057663917541504,
      "learning_rate": 1.9436078979418584e-05,
      "loss": 0.1196,
      "step": 1385
    },
    {
      "epoch": 0.9884444444444445,
      "grad_norm": 4.411609172821045,
      "learning_rate": 1.9427835824037852e-05,
      "loss": 0.1313,
      "step": 1390
    },
    {
      "epoch": 0.992,
      "grad_norm": 36.27834701538086,
      "learning_rate": 1.941953463139517e-05,
      "loss": 0.195,
      "step": 1395
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 30.562034606933594,
      "learning_rate": 1.9411175452592238e-05,
      "loss": 0.1165,
      "step": 1400
    },
    {
      "epoch": 0.9955555555555555,
      "eval_accuracy": 0.9722,
      "eval_f1": 0.9722499500898383,
      "eval_loss": 0.08078985661268234,
      "eval_precision": 0.9705061777600638,
      "eval_recall": 0.974,
      "eval_roc_auc": 0.9960774400000001,
      "eval_runtime": 738.8252,
      "eval_samples_per_second": 6.768,
      "eval_steps_per_second": 0.424,
      "step": 1400
    }
  ],
  "logging_steps": 5,
  "max_steps": 7035,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 140,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.001
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.068539779743744e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
